# 3D Model Generation from Text or Image using Shap-E & Depth Estimation

This project allows you to generate **3D models from either a text prompt or a 2D image** using cutting-edge models like [OpenAI's Shap-E](https://github.com/openai/shap-e) and Intel's DPT for depth estimation. The generated models can be visualized using the included `display_model.py`.


## üìÅ Files

* `main.py`: Core script to generate 3D models from either a **text prompt** or an **image**.
* `display_model.py`: Visualizes the `.obj` 3D models generated by `main.py`.


## üöÄ Features

* ‚úÖ Generate 3D mesh from a text prompt using **Shap-E**
* ‚úÖ Estimate depth from image and convert to 3D mesh
* ‚úÖ Uses **Hugging Face Accelerate** for device-aware execution (CPU/GPU, mixed precision)
* ‚úÖ Save model to `.obj` format for easy reuse and visualization


## üîß Installation

1. **Clone the repo:**

   ```bash
   git clone https://github.com/shaurya-patil/Text-Image-to-3D.git
   cd Text-Image-to-3D
   ```

2. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

3. *(Optional but recommended)* Create a virtual environment:

   ```bash
   python -m venv 3D_Model
   source 3D_Model/bin/activate  # on Linux/Mac
   3D_Model\Scripts\activate     # on Windows
   ```


## üß† Usage

Run the main script to launch the CLI:

```bash
python main.py
```

You will be prompted to choose an input method:

```
Choose an option:
1. Text-to-3D
2. Image-to-3D
```

---

### üìù Option 1: Text-to-3D

* After selecting `1`, you'll be asked to enter a text prompt.
* Example:

```
Enter prompt: an ergonomic design of a chair
```

* The generated 3D model will be saved as:

```
output.obj
```

---

### üñºÔ∏è Option 2: Image-to-3D

* After selecting `2`, the tool will use the image specified in `display_model.py` (e.g., `input_image.jpg`).
* The 3D model will be saved as:

```
output_from_image.obj
```

> üìå **Note:** Ensure the input image file exists and the path is correctly defined in `display_model.py`.

---

Let me know if you'd like to add sections for installation, environment setup, or examples with screenshots or renders.



### Visualize the Model

```bash
python display_model.py output.obj
```

## Output
### Text to 3D
#### Input: "Chair"
![Output](https://github.com/shaurya-patil/Text-Image-to-3D/blob/main/assets/chair-text-output.gif)
#### Input: "White Football"
![Output](https://github.com/shaurya-patil/Text-Image-to-3D/blob/main/assets/football-text-output.gif)
#### Input: "Toy Car"
![Output](https://github.com/shaurya-patil/Text-Image-to-3D/blob/main/assets/toy-car-text-output.gif)

### Image to 3D
#### Input: sedan.jpg
![Output](https://github.com/shaurya-patil/Text-Image-to-3D/blob/main/assets/sedan-image-output.gif)
#### Input: man.jpg
![Output](https://github.com/shaurya-patil/Text-Image-to-3D/blob/main/assets/man-image-output.gif)

You can also run this inside a Jupyter notebook if you're using a 3D viewer like `trimesh`, `vedo`, or `pythreejs`.


## üì¶ Requirements

These are specified in `requirements.txt`, but key libraries include:

* `diffusers`
* `transformers`
* `trimesh`
* `Pillow`
* `scipy`
* `torch`
* `accelerate`


## ‚ö†Ô∏è Note

TripoSR was considered for this project but is currently not compatible with this pipeline-based approach. It is primarily designed as an online web application, not a programmatic, local-use library.

The 2D-to-3D models generated using shap-e or intel/dpt-large are basic and not highly detailed. For high-fidelity and photorealistic 3D outputs, more GPU-intensive solutions like TripoSR or DreamFusion are recommended.
